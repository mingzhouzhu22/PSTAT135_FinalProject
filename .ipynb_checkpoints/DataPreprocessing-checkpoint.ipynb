{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSTAT 135 Final Project: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global imports\n",
    "import pyspark\n",
    "from pyspark.sql.session import SparkSession\n",
    "import pyspark.sql.functions as W\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "# transformations\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "# text transformations\n",
    "import contractions\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "tweets = spark.read.csv('Tweets.csv', header=True, inferSchema=True)\n",
    "# output dataframe\n",
    "tweets.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dimensions of the dataset\n",
    "print(\"Number of rows: \",tweets.count())\n",
    "print(\"Number of columns: \",len(tweets.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output data types of each column\n",
    "tweets.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates\n",
    "\n",
    "- Drop exact duplicate rows\n",
    "- Drop rows that differ only by `tweet_id` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of distinct observations\n",
    "tweets.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "tweets = tweets.dropDuplicates()\n",
    "# confirm they were dropped\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of distinct rows excluding id\n",
    "tweets.select([c for c in tweets.columns if c!='tweet_id']).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output values that are duplicates for all columns except tweet_id\n",
    "tweets.groupBy('airline_sentiment','airline_sentiment_confidence','negativereason',\n",
    "               'negativereason_confidence','airline','airline_sentiment_gold',\n",
    "               'name','negativereason_gold','retweet_count','text','tweet_coord',\n",
    "               'tweet_created','tweet_location','user_timezone')\\\n",
    "    .count()\\\n",
    "    .where(W.col('count')>1)\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates for rows excluding id\n",
    "tweets = tweets.dropDuplicates(subset=[c for c in tweets.columns if c!='tweet_id'])\n",
    "# confirm duplicates were dropped\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "- Remove all rows with missing values for `text`\n",
    "- Ignore missing values later for columns used in exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of missing values for each column\n",
    "tweets.select(*(W.sum(W.col(c).isNull().cast('int')).alias(c) for c in tweets.columns))\\\n",
    "      .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where text is missing\n",
    "tweets = tweets.dropna(subset='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct Categorical Data\n",
    "\n",
    "`airline_sentiment`\n",
    "- change `neutral` to `positive` to create a binary classification problem\n",
    "\n",
    "`negativereason`\n",
    "- change `null` to `Can't Tell`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output unique values for each categorical variable\n",
    "cat_vars = ['airline_sentiment','negativereason','airline','tweet_location','user_timezone']\n",
    "for col in cat_vars:\n",
    "    tweets.select(col).distinct().show(20,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert neutral airline tweets to positive\n",
    "tweets = tweets.withColumn('label', W.when(W.col('airline_sentiment')=='neutral','positive')\\\n",
    "                                          .otherwise(W.col('airline_sentiment')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change missing values for negative reason to \"Can't Tell\"\n",
    "tweets = tweets.fillna({'negativereason':\"Can't Tell\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index Column\n",
    "\n",
    "Used later when combining data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create index column\n",
    "tweets = tweets.withColumn('id', W.monotonically_increasing_id())\n",
    "# output data frame\n",
    "tweets.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Desired Columns\n",
    "\n",
    "Columns will be used later for exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns\n",
    "tweets = tweets.select('id', 'label','negativereason','airline',\n",
    "                       'retweet_count','text','tweet_created')\n",
    "# output dataframe\n",
    "tweets.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns\n",
    "text_df = tweets.select('id','label','text')\n",
    "# output dataframe\n",
    "text_df.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for expanding contractions\n",
    "def fix_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "# udf for expanding contractions\n",
    "contractions_udf = W.udf(lambda row: fix_contractions(row) , StringType())\n",
    "# add column with contractions expanded\n",
    "text_df = text_df.withColumn('text_clean', contractions_udf('text'))\n",
    "# output data frame\n",
    "text_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text, @():;,.!?\\-\\/\"\n",
    "rt = RegexTokenizer().setInputCol('text_clean')\\\n",
    "                     .setOutputCol('text_vec')\\\n",
    "                     .setPattern('\\s+|[\\W]')\\\n",
    "                     .setToLowercase(True)\n",
    "# transform data\n",
    "text_df = rt.transform(text_df)\n",
    "# output dataframe\n",
    "text_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Digits to Words\n",
    "\n",
    "Convert `0` through `9` to `zero` through `9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of number and word keys\n",
    "num_word_dict = {'0':'zero','1':'one','2':'two','3':'three','4':'four',\n",
    "                 '5':'five','6':'six','7':'seven','8':'eight','9':'nine'}\n",
    "# function for converting number to word\n",
    "def num_to_word(row):\n",
    "    new_row = []\n",
    "    for x in row:\n",
    "        if x in num_word_dict.keys():\n",
    "            new_row.append(num_word_dict[x])\n",
    "        else:\n",
    "            new_row.append(x)\n",
    "    return new_row\n",
    "# udf for converting number to word\n",
    "num_to_word_udf = W.udf(lambda row: num_to_word(row) , ArrayType(StringType()))\n",
    "# add column with numbers converted to word\n",
    "text_df = text_df.withColumn('text_vec_num', num_to_word_udf('text_vec'))\n",
    "# output dataframe\n",
    "text_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create english stopwords\n",
    "english = StopWordsRemover().loadDefaultStopWords('english')\n",
    "# stopwords transformer\n",
    "stops = StopWordsRemover().setStopWords(english)\\\n",
    "                          .setInputCol('text_vec_num')\\\n",
    "                          .setOutputCol('text_vec_stop')\n",
    "# transform dataframe\n",
    "text_df = stops.transform(text_df)\n",
    "# output dataframe\n",
    "text_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create word lemmatizer object\n",
    "wnl = WordNetLemmatizer()\n",
    "# function for lemmatizing words\n",
    "def wnl_row(row):\n",
    "    return [wnl.lemmatize(x) for x in row]\n",
    "# udf for lemmatizing words\n",
    "lemmatizer_udf = W.udf(lambda row: wnl_row(row) , ArrayType(StringType()))\n",
    "# create column of lemmatized words\n",
    "text_df = text_df.withColumn('text_vec_lem', lemmatizer_udf('text_vec_stop'))\n",
    "# output dataframe\n",
    "text_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for removing characters\n",
    "def not_letter(row):\n",
    "    return [x for x in row if len(x)>1]\n",
    "# udf for removing characters\n",
    "not_letter_udf = W.udf(lambda row: not_letter(row) , ArrayType(StringType()))\n",
    "# create column with characters removed\n",
    "text_df = text_df.withColumn('text_vec_clean', not_letter_udf('text_vec_lem'))\n",
    "# output dataframe\n",
    "text_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Words\n",
    "\n",
    "Create a vocabulary of words that appear in 0.5% of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transformer\n",
    "cv = CountVectorizer().setInputCol('text_vec_clean')\\\n",
    "                      .setOutputCol('count_vec')\\\n",
    "                      .setMinDF(text_df.count()*0.005)\n",
    "# fit the transformer\n",
    "fit_cv = cv.fit(text_df)\n",
    "# transform the data\n",
    "text_df = fit_cv.transform(text_df)\n",
    "# output the vectorized column\n",
    "text_df.select('count_vec').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary variable\n",
    "vocab = fit_cv.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Label Column to Float\n",
    "\n",
    "`negative` becomes `0` and `positive` becomes `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create string indexer\n",
    "indxr = StringIndexer().setInputCol('label')\\\n",
    "                       .setOutputCol('label_idx')\n",
    "# transform data\n",
    "text_df = indxr.fit(text_df).transform(text_df)\n",
    "# output dataframe\n",
    "text_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Datasets\n",
    "\n",
    "70% of the data is used to train the model, 30% of the data is used to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test sets\n",
    "train,test = text_df.randomSplit([0.7,0.3],116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output train data\n",
    "train.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Dataset for Exploratory Data Analysis\n",
    "\n",
    "Make a dataset for the training data with additional features for exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tweets and train to get a data frame for exploratory data analysis with additional features\n",
    "tweets_train = tweets.join(train, on=['id'], how='left_semi')\n",
    "tweets_train.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Columns for Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns for train and test\n",
    "train = train.select('count_vec','label_idx')\n",
    "test = test.select('count_vec','label_idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data Frames for Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store dataframes for additional programs\n",
    "dfs = [train.toPandas(),test.toPandas(),tweets_train.toPandas(),vocab]\n",
    "%store dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
