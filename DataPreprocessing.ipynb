{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSTAT 135 Final Project: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global imports\n",
    "import pyspark\n",
    "from pyspark.sql.session import SparkSession\n",
    "import pyspark.sql.functions as W\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "# transformations\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "# text transformations\n",
    "import contractions\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, NGram, CountVectorizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+----------------------------+--------------+-------------------------+--------------+----------------------+--------+-------------------+-------------+--------------------+-----------+--------------------+--------------+--------------------+\n",
      "|          tweet_id|airline_sentiment|airline_sentiment_confidence|negativereason|negativereason_confidence|       airline|airline_sentiment_gold|    name|negativereason_gold|retweet_count|                text|tweet_coord|       tweet_created|tweet_location|       user_timezone|\n",
      "+------------------+-----------------+----------------------------+--------------+-------------------------+--------------+----------------------+--------+-------------------+-------------+--------------------+-----------+--------------------+--------------+--------------------+\n",
      "|570306133677760513|          neutral|                         1.0|          null|                     null|Virgin America|                  null| cairdin|               null|            0|@VirginAmerica Wh...|       null|2015-02-24 11:35:...|          null|Eastern Time (US ...|\n",
      "|570301130888122368|         positive|                      0.3486|          null|                      0.0|Virgin America|                  null|jnardino|               null|            0|@VirginAmerica pl...|       null|2015-02-24 11:15:...|          null|Pacific Time (US ...|\n",
      "+------------------+-----------------+----------------------------+--------------+-------------------------+--------------+----------------------+--------+-------------------+-------------+--------------------+-----------+--------------------+--------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "tweets = spark.read.csv('Tweets.csv', header=True, inferSchema=True)\n",
    "# output dataframe\n",
    "tweets.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  14837\n",
      "Number of columns:  15\n"
     ]
    }
   ],
   "source": [
    "# output dimensions of the dataset\n",
    "print(\"Number of rows: \",tweets.count())\n",
    "print(\"Number of columns: \",len(tweets.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tweet_id', 'string'),\n",
       " ('airline_sentiment', 'string'),\n",
       " ('airline_sentiment_confidence', 'string'),\n",
       " ('negativereason', 'string'),\n",
       " ('negativereason_confidence', 'string'),\n",
       " ('airline', 'string'),\n",
       " ('airline_sentiment_gold', 'string'),\n",
       " ('name', 'string'),\n",
       " ('negativereason_gold', 'string'),\n",
       " ('retweet_count', 'int'),\n",
       " ('text', 'string'),\n",
       " ('tweet_coord', 'string'),\n",
       " ('tweet_created', 'string'),\n",
       " ('tweet_location', 'string'),\n",
       " ('user_timezone', 'string')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output data types of each column\n",
    "tweets.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates\n",
    "\n",
    "- Drop exact duplicate rows\n",
    "- Drop rows that differ only by `tweet_id` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14785"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of distinct observations\n",
    "tweets.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14785"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicates\n",
    "tweets = tweets.dropDuplicates()\n",
    "# confirm they were dropped\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14751"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of distinct rows excluding id\n",
    "tweets.select([c for c in tweets.columns if c!='tweet_id']).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------------------+--------------------+-------------------------+----------+----------------------+---------------+-------------------+-------------+-----------+-----------+-------------+--------------+-------------+-----+\n",
      "|airline_sentiment|airline_sentiment_confidence|      negativereason|negativereason_confidence|   airline|airline_sentiment_gold|           name|negativereason_gold|retweet_count|       text|tweet_coord|tweet_created|tweet_location|user_timezone|count|\n",
      "+-----------------+----------------------------+--------------------+-------------------------+----------+----------------------+---------------+-------------------+-------------+-----------+-----------+-------------+--------------+-------------+-----+\n",
      "|             null|                        null|                null|                     null|      null|                  null|           null|               null|         null|       null|       null|         null|          null|         null|   34|\n",
      "|         negative|                         1.0|Customer Service ...|                      1.0|US Airways|                  null|istackfranklins|               null|            0|@USAirways |       null|         null|          null|         null|    2|\n",
      "+-----------------+----------------------------+--------------------+-------------------------+----------+----------------------+---------------+-------------------+-------------+-----------+-----------+-------------+--------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# output values that are duplicates for all columns except tweet_id\n",
    "tweets.groupBy('airline_sentiment','airline_sentiment_confidence','negativereason',\n",
    "               'negativereason_confidence','airline','airline_sentiment_gold',\n",
    "               'name','negativereason_gold','retweet_count','text','tweet_coord',\n",
    "               'tweet_created','tweet_location','user_timezone')\\\n",
    "    .count()\\\n",
    "    .where(W.col('count')>1)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14751"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicates for rows excluding id\n",
    "tweets = tweets.dropDuplicates(subset=[c for c in tweets.columns if c!='tweet_id'])\n",
    "# confirm duplicates were dropped\n",
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "- Remove all rows with missing values for `text`\n",
    "- Ignore missing values later for columns used in exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+----------------------------+--------------+-------------------------+-------+----------------------+----+-------------------+-------------+----+-----------+-------------+--------------+-------------+\n",
      "|tweet_id|airline_sentiment|airline_sentiment_confidence|negativereason|negativereason_confidence|airline|airline_sentiment_gold|name|negativereason_gold|retweet_count|text|tweet_coord|tweet_created|tweet_location|user_timezone|\n",
      "+--------+-----------------+----------------------------+--------------+-------------------------+-------+----------------------+----+-------------------+-------------+----+-----------+-------------+--------------+-------------+\n",
      "|       0|              107|                          21|          5508|                     4164|    131|                 14702| 147|              14719|          156| 156|      13686|          338|          4949|         5046|\n",
      "+--------+-----------------+----------------------------+--------------+-------------------------+-------+----------------------+----+-------------------+-------------+----+-----------+-------------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of missing values for each column\n",
    "tweets.select(*(W.sum(W.col(c).isNull().cast('int')).alias(c) for c in tweets.columns))\\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where text is missing\n",
    "tweets = tweets.dropna(subset='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Categorical Data\n",
    "\n",
    "`airline_sentiment`\n",
    "- change `neutral` to `positive` to create a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|airline_sentiment|\n",
      "+-----------------+\n",
      "|positive         |\n",
      "|neutral          |\n",
      "|negative         |\n",
      "+-----------------+\n",
      "\n",
      "+---------------------------+\n",
      "|negativereason             |\n",
      "+---------------------------+\n",
      "|Lost Luggage               |\n",
      "|longlines                  |\n",
      "|Late Flight                |\n",
      "|null                       |\n",
      "|Damaged Luggage            |\n",
      "|Cancelled Flight           |\n",
      "|Customer Service Issue     |\n",
      "|Flight Attendant Complaints|\n",
      "|Bad Flight                 |\n",
      "|Can't Tell                 |\n",
      "|Flight Booking Problems    |\n",
      "+---------------------------+\n",
      "\n",
      "+--------------+\n",
      "|airline       |\n",
      "+--------------+\n",
      "|Delta         |\n",
      "|Virgin America|\n",
      "|United        |\n",
      "|US Airways    |\n",
      "|Southwest     |\n",
      "|American      |\n",
      "+--------------+\n",
      "\n",
      "+--------------------------+\n",
      "|tweet_location            |\n",
      "+--------------------------+\n",
      "|washington, dc            |\n",
      "|✖️ || 4/5 || ✖️           |\n",
      "|Winchester | Northend Ma  |\n",
      "|Heart of America          |\n",
      "|Bangalore                 |\n",
      "|Gone West                 |\n",
      "|Downtown Dallas, Texas    |\n",
      "|Utah                      |\n",
      "|Melbourne, Aus            |\n",
      "|Winnipeg, MB Canada       |\n",
      "|Scituate, MA              |\n",
      "|Brooklyn NY               |\n",
      "|Wilmington, North Carolina|\n",
      "|ʞᴚoʎ   ʍǝu                |\n",
      "|Ho-Flo/Columbus/NYC       |\n",
      "|Alexandria, Va            |\n",
      "|Coming to a City Near You!|\n",
      "|Port Washington, NY 11050 |\n",
      "|@ the moment in New Delhi |\n",
      "|Novi, MI                  |\n",
      "+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------------+\n",
      "|user_timezone             |\n",
      "+--------------------------+\n",
      "|Hawaii                    |\n",
      "|Casablanca                |\n",
      "|Wellington                |\n",
      "|Lima                      |\n",
      "|Central Time (US & Canada)|\n",
      "|Madrid                    |\n",
      "|Prague                    |\n",
      "|Canberra                  |\n",
      "|North Hollywood, CA       |\n",
      "|Edinburgh                 |\n",
      "|Singapore                 |\n",
      "|Adelaide                  |\n",
      "|2015-02-23 16:52:54 -0800 |\n",
      "|America/Atikokan          |\n",
      "|America/Chicago           |\n",
      "|Columbus, OH              |\n",
      "|Beijing                   |\n",
      "|DC                        |\n",
      "|Bern                      |\n",
      "|Arkansas                  |\n",
      "+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# output unique values for each categorical variable\n",
    "cat_vars = ['airline_sentiment','negativereason','airline','tweet_location','user_timezone']\n",
    "for col in cat_vars:\n",
    "    tweets.select(col).distinct().show(20,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert neutral airline tweets to positive\n",
    "tweets = tweets.withColumn('label', W.when(W.col('airline_sentiment')=='neutral','positive')\\\n",
    "                                          .otherwise(W.col('airline_sentiment')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index Column\n",
    "\n",
    "Used later when combining data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+----------------------------+--------------+-------------------------+--------+----------------------+-----------+-------------------+-------------+--------------------+-----------+--------------------+--------------+-------------+--------+---+\n",
      "|          tweet_id|airline_sentiment|airline_sentiment_confidence|negativereason|negativereason_confidence| airline|airline_sentiment_gold|       name|negativereason_gold|retweet_count|                text|tweet_coord|       tweet_created|tweet_location|user_timezone|   label| id|\n",
      "+------------------+-----------------+----------------------------+--------------+-------------------------+--------+----------------------+-----------+-------------------+-------------+--------------------+-----------+--------------------+--------------+-------------+--------+---+\n",
      "|569912710848057344|         negative|                         1.0|   Late Flight|                      1.0|  United|                  null|ADanaiBaker|               null|            0|@united Now arriv...|       null|2015-02-23 09:32:...|          null|         null|negative|  0|\n",
      "|569941858132070400|         negative|                         1.0|    Bad Flight|                   0.6404|American|                  null|   Aero0729|               null|            0|@AmericanAir don'...|       null|2015-02-23 11:28:...|          null|         null|negative|  1|\n",
      "+------------------+-----------------+----------------------------+--------------+-------------------------+--------+----------------------+-----------+-------------------+-------------+--------------------+-----------+--------------------+--------------+-------------+--------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create index column\n",
    "tweets = tweets.withColumn('id', W.monotonically_increasing_id())\n",
    "# output data frame\n",
    "tweets.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Desired Columns\n",
    "\n",
    "Columns will be used later for exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+----------+-------------+--------------------+\n",
      "| id|   label|      negativereason|   airline|retweet_count|                text|\n",
      "+---+--------+--------------------+----------+-------------+--------------------+\n",
      "|  0|negative|Flight Booking Pr...|US Airways|            0|@USAirways but wa...|\n",
      "|  1|negative|          Bad Flight|US Airways|            0|@USAirways yes.  ...|\n",
      "+---+--------+--------------------+----------+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select columns\n",
    "tweets = tweets.select('id','label','negativereason','airline','retweet_count','text')\n",
    "# output dataframe\n",
    "tweets.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  14595\n",
      "Number of columns:  6\n"
     ]
    }
   ],
   "source": [
    "# output dimensions of the dataset\n",
    "print(\"Number of rows: \",tweets.count())\n",
    "print(\"Number of columns: \",len(tweets.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+\n",
      "| id|   label|                text|\n",
      "+---+--------+--------------------+\n",
      "|  0|negative|@USAirways but wa...|\n",
      "|  1|negative|@USAirways yes.  ...|\n",
      "+---+--------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select columns\n",
    "text_df = tweets.select('id','label','text')\n",
    "# output dataframe\n",
    "text_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+\n",
      "| id|   label|                text|          text_clean|\n",
      "+---+--------+--------------------+--------------------+\n",
      "|  0|negative|@USAirways but wa...|@USAirways but wa...|\n",
      "|  1|negative|@USAirways yes.  ...|@USAirways yes.  ...|\n",
      "+---+--------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function for expanding contractions\n",
    "def fix_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "# udf for expanding contractions\n",
    "contractions_udf = W.udf(lambda row: fix_contractions(row) , StringType())\n",
    "# add column with contractions expanded\n",
    "text_df = text_df.withColumn('text_clean', contractions_udf('text'))\n",
    "# output data frame\n",
    "text_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+\n",
      "| id|   label|                text|          text_clean|            text_vec|\n",
      "+---+--------+--------------------+--------------------+--------------------+\n",
      "|  0|negative|@USAirways but wa...|@USAirways but wa...|[usairways, but, ...|\n",
      "|  1|negative|@USAirways yes.  ...|@USAirways yes.  ...|[usairways, yes, ...|\n",
      "+---+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenize the text, @():;,.!?\\-\\/\"\n",
    "rt = RegexTokenizer().setInputCol('text_clean')\\\n",
    "                     .setOutputCol('text_vec')\\\n",
    "                     .setPattern('\\s+|[\\W]')\\\n",
    "                     .setToLowercase(True)\n",
    "# transform data\n",
    "text_df = rt.transform(text_df)\n",
    "# output dataframe\n",
    "text_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Digits to Words\n",
    "\n",
    "Convert `0` through `9` to `zero` through `9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|   label|                text|          text_clean|            text_vec|        text_vec_num|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|negative|@USAirways but wa...|@USAirways but wa...|[usairways, but, ...|[usairways, but, ...|\n",
      "|  1|negative|@USAirways yes.  ...|@USAirways yes.  ...|[usairways, yes, ...|[usairways, yes, ...|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dictionary of number and word keys\n",
    "num_word_dict = {'0':'zero','1':'one','2':'two','3':'three','4':'four',\n",
    "                 '5':'five','6':'six','7':'seven','8':'eight','9':'nine'}\n",
    "# function for converting number to word\n",
    "def num_to_word(row):\n",
    "    new_row = []\n",
    "    for x in row:\n",
    "        if x in num_word_dict.keys():\n",
    "            new_row.append(num_word_dict[x])\n",
    "        else:\n",
    "            new_row.append(x)\n",
    "    return new_row\n",
    "# udf for converting number to word\n",
    "num_to_word_udf = W.udf(lambda row: num_to_word(row) , ArrayType(StringType()))\n",
    "# add column with numbers converted to word\n",
    "text_df = text_df.withColumn('text_vec_num', num_to_word_udf('text_vec'))\n",
    "# output dataframe\n",
    "text_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|   label|                text|          text_clean|            text_vec|        text_vec_num|       text_vec_stop|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|negative|@USAirways but wa...|@USAirways but wa...|[usairways, but, ...|[usairways, but, ...|[usairways, wait,...|\n",
      "|  1|negative|@USAirways yes.  ...|@USAirways yes.  ...|[usairways, yes, ...|[usairways, yes, ...|[usairways, yes, ...|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create english stopwords\n",
    "english = StopWordsRemover().loadDefaultStopWords('english')\n",
    "# stopwords transformer\n",
    "stops = StopWordsRemover().setStopWords(english)\\\n",
    "                          .setInputCol('text_vec_num')\\\n",
    "                          .setOutputCol('text_vec_stop')\n",
    "# transform dataframe\n",
    "text_df = stops.transform(text_df)\n",
    "# output dataframe\n",
    "text_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|   label|                text|          text_clean|            text_vec|        text_vec_num|       text_vec_stop|        text_vec_lem|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|negative|@USAirways but wa...|@USAirways but wa...|[usairways, but, ...|[usairways, but, ...|[usairways, wait,...|[usairways, wait,...|\n",
      "|  1|negative|@USAirways yes.  ...|@USAirways yes.  ...|[usairways, yes, ...|[usairways, yes, ...|[usairways, yes, ...|[usairways, yes, ...|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create word lemmatizer object\n",
    "wnl = WordNetLemmatizer()\n",
    "# function for lemmatizing words\n",
    "def wnl_row(row):\n",
    "    return [wnl.lemmatize(x) for x in row]\n",
    "# udf for lemmatizing words\n",
    "lemmatizer_udf = W.udf(lambda row: wnl_row(row) , ArrayType(StringType()))\n",
    "# create column of lemmatized words\n",
    "text_df = text_df.withColumn('text_vec_lem', lemmatizer_udf('text_vec_stop'))\n",
    "# output dataframe\n",
    "text_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|   label|                text|          text_clean|            text_vec|        text_vec_num|       text_vec_stop|        text_vec_lem|      text_vec_clean|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|negative|@USAirways but wa...|@USAirways but wa...|[usairways, but, ...|[usairways, but, ...|[usairways, wait,...|[usairways, wait,...|[usairways, wait,...|\n",
      "|  1|negative|@USAirways yes.  ...|@USAirways yes.  ...|[usairways, yes, ...|[usairways, yes, ...|[usairways, yes, ...|[usairways, yes, ...|[usairways, yes, ...|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function for removing characters\n",
    "def not_letter(row):\n",
    "    return [x for x in row if len(x)>1]\n",
    "# udf for removing characters\n",
    "not_letter_udf = W.udf(lambda row: not_letter(row) , ArrayType(StringType()))\n",
    "# create column with characters removed\n",
    "text_df = text_df.withColumn('text_vec_clean', not_letter_udf('text_vec_lem'))\n",
    "# output dataframe\n",
    "text_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_vec_clean</th>\n",
       "      <th>text_bigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[usairways, wait, booked, along, hotel, nearby...</td>\n",
       "      <td>[usairways wait, wait booked, booked along, al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[usairways, yes, every, one, every, flight, li...</td>\n",
       "      <td>[usairways yes, yes every, every one, one ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[southwestair, retroactively, add, previous, f...</td>\n",
       "      <td>[southwestair retroactively, retroactively add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[sigh, jetblue, fleet, fleek, http, co, w5nl0a...</td>\n",
       "      <td>[sigh jetblue, jetblue fleet, fleet fleek, fle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[jetblue, whoa, tag, still, saw, tweet, flight...</td>\n",
       "      <td>[jetblue whoa, whoa tag, tag still, still saw,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text_vec_clean  \\\n",
       "0  [usairways, wait, booked, along, hotel, nearby...   \n",
       "1  [usairways, yes, every, one, every, flight, li...   \n",
       "2  [southwestair, retroactively, add, previous, f...   \n",
       "3  [sigh, jetblue, fleet, fleek, http, co, w5nl0a...   \n",
       "4  [jetblue, whoa, tag, still, saw, tweet, flight...   \n",
       "\n",
       "                                         text_bigram  \n",
       "0  [usairways wait, wait booked, booked along, al...  \n",
       "1  [usairways yes, yes every, every one, one ever...  \n",
       "2  [southwestair retroactively, retroactively add...  \n",
       "3  [sigh jetblue, jetblue fleet, fleet fleek, fle...  \n",
       "4  [jetblue whoa, whoa tag, tag still, still saw,...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create bigrams\n",
    "bigram = NGram().setInputCol('text_vec_clean')\\\n",
    "                .setOutputCol('text_bigram')\\\n",
    "                .setN(2)\n",
    "# transform the data frame\n",
    "text_df_bigram = bigram.transform(text_df.select('text_vec_clean'))\n",
    "# check it worked\n",
    "text_df_bigram.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|count_vec_bigram                        |\n",
      "+----------------------------------------+\n",
      "|(500,[],[])                             |\n",
      "|(500,[131],[1.0])                       |\n",
      "|(500,[],[])                             |\n",
      "|(500,[0,10,11,12],[1.0,1.0,1.0,1.0])    |\n",
      "|(500,[34,42],[1.0,1.0])                 |\n",
      "|(500,[],[])                             |\n",
      "|(500,[113],[1.0])                       |\n",
      "|(500,[0],[1.0])                         |\n",
      "|(500,[],[])                             |\n",
      "|(500,[24,229],[1.0,1.0])                |\n",
      "|(500,[],[])                             |\n",
      "|(500,[28,467],[1.0,1.0])                |\n",
      "|(500,[],[])                             |\n",
      "|(500,[87,352,376],[1.0,1.0,1.0])        |\n",
      "|(500,[48,87],[1.0,1.0])                 |\n",
      "|(500,[6,120],[1.0,1.0])                 |\n",
      "|(500,[],[])                             |\n",
      "|(500,[],[])                             |\n",
      "|(500,[39,271,354,428],[1.0,1.0,1.0,1.0])|\n",
      "|(500,[],[])                             |\n",
      "+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary of the most common words\n",
    "cv_bigram = CountVectorizer().setInputCol('text_bigram')\\\n",
    "                      .setOutputCol('count_vec_bigram')\\\n",
    "                      .setVocabSize(500)\\\n",
    "                      .setMinTF(1)\\\n",
    "                      .setMinDF(1)\n",
    "# fit the count vectorizer\n",
    "fit_cv_bigram = cv_bigram.fit(text_df_bigram)\n",
    "# transform the data frame\n",
    "bigram_df = fit_cv_bigram.transform(text_df_bigram)\n",
    "# check it worked\n",
    "bigram_df.select('count_vec_bigram').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http co',\n",
       " 'customer service',\n",
       " 'cancelled flightled',\n",
       " 'late flight',\n",
       " 'cancelled flighted',\n",
       " 'two hour',\n",
       " 'flight cancelled',\n",
       " 'cancelled flight',\n",
       " 'late flightr',\n",
       " 'booking problem',\n",
       " 'fleet fleek',\n",
       " 'jetblue fleet',\n",
       " 'fleek http',\n",
       " 'united thanks',\n",
       " 'flightled flight',\n",
       " 'united flight',\n",
       " 'americanair flight',\n",
       " 'flight delayed',\n",
       " 'usairways americanair',\n",
       " 'usairways flight']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output 20 most popular bigrams\n",
    "fit_cv_bigram.vocabulary[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Words\n",
    "\n",
    "Create a vocabulary of words that appear in 0.5% of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------+\n",
      "|count_vec                                                                      |\n",
      "+-------------------------------------------------------------------------------+\n",
      "|(349,[2,58,118,127,152],[1.0,1.0,1.0,1.0,2.0])                                 |\n",
      "|(349,[0,2,16,22,40,96,210,286],[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0])              |\n",
      "|(349,[0,4,134,186,285],[1.0,1.0,1.0,1.0,1.0])                                  |\n",
      "|(349,[5,7,8,158,159],[1.0,1.0,1.0,1.0,1.0])                                    |\n",
      "|(349,[0,5,6,24,54,70,81,114,275,321],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+-------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create transformer\n",
    "cv = CountVectorizer().setInputCol('text_vec_clean')\\\n",
    "                      .setOutputCol('count_vec')\\\n",
    "                      .setMinDF(text_df.count()*0.005)\n",
    "# fit the transformer\n",
    "fit_cv = cv.fit(text_df)\n",
    "# transform the data\n",
    "text_df = fit_cv.transform(text_df)\n",
    "# output the vectorized column\n",
    "text_df.select('count_vec').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary variable\n",
    "vocab = fit_cv.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Label Column to Float\n",
    "\n",
    "`negative` becomes `0` and `positive` becomes `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "| id|   label|                text|          text_clean|            text_vec|        text_vec_num|       text_vec_stop|        text_vec_lem|      text_vec_clean|           count_vec|label_idx|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|  0|negative|@USAirways but wa...|@USAirways but wa...|[usairways, but, ...|[usairways, but, ...|[usairways, wait,...|[usairways, wait,...|[usairways, wait,...|(349,[2,58,118,12...|      0.0|\n",
      "|  1|negative|@USAirways yes.  ...|@USAirways yes.  ...|[usairways, yes, ...|[usairways, yes, ...|[usairways, yes, ...|[usairways, yes, ...|[usairways, yes, ...|(349,[0,2,16,22,4...|      0.0|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create string indexer\n",
    "indxr = StringIndexer().setInputCol('label')\\\n",
    "                       .setOutputCol('label_idx')\n",
    "# transform data\n",
    "text_df = indxr.fit(text_df).transform(text_df)\n",
    "# output dataframe\n",
    "text_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Datasets\n",
    "\n",
    "70% of the data is used to train the model, 30% of the data is used to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test sets\n",
    "train,test = text_df.randomSplit([0.7,0.3],116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "| id|   label|                text|          text_clean|            text_vec|        text_vec_num|       text_vec_stop|        text_vec_lem|      text_vec_clean|           count_vec|label_idx|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|  1|negative|@USAirways yes.  ...|@USAirways yes.  ...|[usairways, yes, ...|[usairways, yes, ...|[usairways, yes, ...|[usairways, yes, ...|[usairways, yes, ...|(349,[0,2,16,22,4...|      0.0|\n",
      "|  3|positive|Sigh... “@JetBlue...|Sigh... “@JetBlue...|[sigh, jetblue, o...|[sigh, jetblue, o...|[sigh, jetblue, f...|[sigh, jetblue, f...|[sigh, jetblue, f...|(349,[5,7,8,158,1...|      1.0|\n",
      "+---+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# output train data\n",
    "train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Dataset for Exploratory Data Analysis\n",
    "\n",
    "Make a dataset for the training data with additional features for exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------+-------------+--------------------+\n",
      "|   label|negativereason|   airline|retweet_count|                text|\n",
      "+--------+--------------+----------+-------------+--------------------+\n",
      "|negative|    Bad Flight|US Airways|            0|@USAirways yes.  ...|\n",
      "|positive|          null|     Delta|            0|Sigh... “@JetBlue...|\n",
      "+--------+--------------+----------+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge tweets and train to get a data frame for exploratory data analysis with additional features\n",
    "tweets_train = tweets.join(train, on=['id'], how='left_semi')\n",
    "tweets_train = tweets_train.select('label','negativereason','airline','retweet_count','text')\n",
    "tweets_train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Columns for Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns for train and test\n",
    "train = train.select('count_vec','label_idx')\n",
    "test = test.select('count_vec','label_idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data Frames for Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'dfs' (list)\n"
     ]
    }
   ],
   "source": [
    "# store dataframes for additional programs\n",
    "dfs = [train.toPandas(),test.toPandas(),tweets_train.toPandas(),vocab]\n",
    "%store dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
