{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Analyze the words with large weights in ols, lasso and ridge\n",
    "- Analyze the important words from RF\n",
    "- Analyze the words for observations that were misclassified under \"Examine Misclassified Words for Positive Tweets\" and \"Examine Missclassified Words for Negative Tweets\"\n",
    "    - I created `pos_counts_dict` and `neg_counts_dict` as starting points (will need to convert into list or data frame depending on what analysis is being done)\n",
    "    - Maybe make a word cloud? Although we already have other word clouds so maybe something else? Not sure what are good visualizations for words so maybe google that?\n",
    "    - Also on that note, if you find a visualization that would make more sense to do instead of a word cloud for any of the word clouds in DataExploration.ipynb, please feel free to do that :) \n",
    "- Do similar analysis as above with the correctly classified stuff\n",
    "- Add more to the proportion stuff for both above since I just made the dataframe but then didn't do much after that\n",
    "- Try to do something with `lsvc_test`? IDK what to do \n",
    "- Anything else you can think of :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Machine Learning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global imports\n",
    "import pyspark\n",
    "from pyspark.sql.session import SparkSession\n",
    "import pyspark.sql.functions as W\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "%store -r ml_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+----------+\n",
      "|           count_vec|label_idx|       rawPrediction|prediction|\n",
      "+--------------------+---------+--------------------+----------+\n",
      "|(349,[2,58,118,12...|      0.0|[0.76933931432304...|       0.0|\n",
      "|(349,[0,4,134,186...|      1.0|[-0.4556061437591...|       1.0|\n",
      "+--------------------+---------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# linear support vector machine results\n",
    "lsvc_test = spark.createDataFrame(ml_dfs[0])\n",
    "lsvc_test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+---+--------+-------+--------+----------+----------+---------+\n",
      "|           count_vec|label_idx|rf_pred| id|gbt_pred|nb_pred|ols_pred|lasso_pred|ridge_pred|lsvc_pred|\n",
      "+--------------------+---------+-------+---+--------+-------+--------+----------+----------+---------+\n",
      "|(349,[2,58,118,12...|      0.0|    0.0|  1|     0.0|    0.0|     0.0|       0.0|       0.0|      0.0|\n",
      "|(349,[0,4,134,186...|      1.0|    0.0|  2|     1.0|    1.0|     1.0|       1.0|       1.0|      1.0|\n",
      "+--------------------+---------+-------+---+--------+-------+--------+----------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all machine learning results\n",
    "results = spark.createDataFrame(ml_dfs[1])\n",
    "results.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "%store -r dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vocabulary\n",
    "vocab = dfs[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misclassified Observations by Every Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+---+--------+-------+--------+----------+----------+---------+\n",
      "|           count_vec|label_idx|rf_pred| id|gbt_pred|nb_pred|ols_pred|lasso_pred|ridge_pred|lsvc_pred|\n",
      "+--------------------+---------+-------+---+--------+-------+--------+----------+----------+---------+\n",
      "|(349,[1,110,280,3...|      1.0|    0.0|  3|     0.0|    0.0|     0.0|       0.0|       0.0|      0.0|\n",
      "|(349,[2,11,13,15,...|      1.0|    0.0| 20|     0.0|    0.0|     0.0|       0.0|       0.0|      0.0|\n",
      "+--------------------+---------+-------+---+--------+-------+--------+----------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataframe of observations that were always misclassified\n",
    "misclassified = results.where((W.col('label_idx')!=W.col('rf_pred')) &\n",
    "                             (W.col('label_idx')!=W.col('gbt_pred')) &\n",
    "                             (W.col('label_idx')!=W.col('nb_pred')) &\n",
    "                             (W.col('label_idx')!=W.col('ols_pred')) &\n",
    "                             (W.col('label_idx')!=W.col('lasso_pred')) &\n",
    "                             (W.col('label_idx')!=W.col('ridge_pred')) &\n",
    "                             (W.col('label_idx')!=W.col('lsvc_pred')))\n",
    "# output dataframe\n",
    "misclassified.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Misclassified Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|label_idx|count|\n",
      "+---------+-----+\n",
      "|      0.0| 2779|\n",
      "|      1.0| 1593|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get total counts\n",
    "total_counts = results.groupby('label_idx').count()\n",
    "total_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+--------------------+\n",
      "|miscount|label_idx|count|          proportion|\n",
      "+--------+---------+-----+--------------------+\n",
      "|      19|      0.0| 2779|0.006836991723641598|\n",
      "|     349|      1.0| 1593| 0.21908349026993096|\n",
      "+--------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get proportion misclassified\n",
    "mis_counts = misclassified.groupby('label_idx').count()\n",
    "mis_counts = mis_counts.withColumnRenamed('label_idx','label_idx2')\n",
    "mis_counts = mis_counts.withColumnRenamed('count','miscount')\n",
    "mis_counts = mis_counts.join(total_counts,total_counts.label_idx==mis_counts.label_idx2)\n",
    "mis_counts = mis_counts.drop('label_idx2')\n",
    "mis_counts = mis_counts.withColumn('proportion',W.col('miscount')/W.col('count'))\n",
    "mis_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Misclassified Words for Positive Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for creating key and value pair of a row\n",
    "def key_val(row):\n",
    "    new_row = [(i,row[i]) for i in range(len(row))]\n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of indices and word counts\n",
    "mis_pos_counts_dict = misclassified.where(W.col('label_idx')==1)\\\n",
    "                                   .select('count_vec').rdd\\\n",
    "                                   .map(lambda row: row.count_vec.toArray())\\\n",
    "                                   .map(lambda row: key_val(row))\\\n",
    "                                   .flatMap(lambda row: row)\\\n",
    "                                   .reduceByKey(lambda x,y: x+y)\\\n",
    "                                   .mapValues(lambda x: int(x))\\\n",
    "                                   .collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the dictionary so the keys correspond to words\n",
    "for i in range(0,len(vocab)):\n",
    "    mis_pos_counts_dict[vocab[i]] = mis_pos_counts_dict.pop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Misclassified Words for Negative Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of indices and word counts\n",
    "mis_neg_counts_dict = misclassified.where(W.col('label_idx')==0)\\\n",
    "                                   .select('count_vec').rdd\\\n",
    "                                   .map(lambda row: row.count_vec.toArray())\\\n",
    "                                   .map(lambda row: key_val(row))\\\n",
    "                                   .flatMap(lambda row: row)\\\n",
    "                                   .reduceByKey(lambda x,y: x+y)\\\n",
    "                                   .mapValues(lambda x: int(x))\\\n",
    "                                   .collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the dictionary so the keys correspond to words\n",
    "for i in range(0,len(vocab)):\n",
    "    mis_neg_counts_dict[vocab[i]] = mis_neg_counts_dict.pop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correctly Classified Observations by Every Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+---+--------+-------+--------+----------+----------+---------+\n",
      "|           count_vec|label_idx|rf_pred| id|gbt_pred|nb_pred|ols_pred|lasso_pred|ridge_pred|lsvc_pred|\n",
      "+--------------------+---------+-------+---+--------+-------+--------+----------+----------+---------+\n",
      "|(349,[2,58,118,12...|      0.0|    0.0|  1|     0.0|    0.0|     0.0|       0.0|       0.0|      0.0|\n",
      "|(349,[1,6,13,18,2...|      0.0|    0.0|  4|     0.0|    0.0|     0.0|       0.0|       0.0|      0.0|\n",
      "+--------------------+---------+-------+---+--------+-------+--------+----------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataframe of observations that were always misclassified\n",
    "correct = results.where((W.col('label_idx')==W.col('rf_pred')) &\n",
    "                             (W.col('label_idx')==W.col('gbt_pred')) &\n",
    "                             (W.col('label_idx')==W.col('nb_pred')) &\n",
    "                             (W.col('label_idx')==W.col('ols_pred')) &\n",
    "                             (W.col('label_idx')==W.col('lasso_pred')) &\n",
    "                             (W.col('label_idx')==W.col('ridge_pred')) &\n",
    "                             (W.col('label_idx')==W.col('lsvc_pred')))\n",
    "# output dataframe\n",
    "correct.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Correctly Classified Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+-------------------+\n",
      "|miscount|label_idx|count|         proportion|\n",
      "+--------+---------+-----+-------------------+\n",
      "|    2130|      0.0| 2779| 0.7664627563871896|\n",
      "|     214|      1.0| 1593|0.13433772755806653|\n",
      "+--------+---------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get proportion misclassified\n",
    "correct_counts = correct.groupby('label_idx').count()\n",
    "correct_counts = correct_counts.withColumnRenamed('label_idx','label_idx2')\n",
    "correct_counts = correct_counts.withColumnRenamed('count','miscount')\n",
    "correct_counts = correct_counts.join(total_counts,total_counts.label_idx==correct_counts.label_idx2)\n",
    "correct_counts = correct_counts.drop('label_idx2')\n",
    "correct_counts = correct_counts.withColumn('proportion',W.col('miscount')/W.col('count'))\n",
    "correct_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Correctly Classified Words for Positive Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of indices and word counts\n",
    "correct_pos_counts_dict = correct.where(W.col('label_idx')==1)\\\n",
    "                                 .select('count_vec').rdd\\\n",
    "                                 .map(lambda row: row.count_vec.toArray())\\\n",
    "                                 .map(lambda row: key_val(row))\\\n",
    "                                 .flatMap(lambda row: row)\\\n",
    "                                 .reduceByKey(lambda x,y: x+y)\\\n",
    "                                 .mapValues(lambda x: int(x))\\\n",
    "                                 .collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the dictionary so the keys correspond to words\n",
    "for i in range(0,len(vocab)):\n",
    "    correct_pos_counts_dict[vocab[i]] = correct_pos_counts_dict.pop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Correctly Classified Words for Negative Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of indices and word counts\n",
    "correct_neg_counts_dict = correct.where(W.col('label_idx')==0)\\\n",
    "                                 .select('count_vec').rdd\\\n",
    "                                 .map(lambda row: row.count_vec.toArray())\\\n",
    "                                 .map(lambda row: key_val(row))\\\n",
    "                                 .flatMap(lambda row: row)\\\n",
    "                                 .reduceByKey(lambda x,y: x+y)\\\n",
    "                                 .mapValues(lambda x: int(x))\\\n",
    "                                 .collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the dictionary so the keys correspond to words\n",
    "for i in range(0,len(vocab)):\n",
    "    correct_neg_counts_dict[vocab[i]] = correct_neg_counts_dict.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
